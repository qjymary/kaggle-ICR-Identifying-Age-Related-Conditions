{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ccd0ba",
   "metadata": {},
   "source": [
    "# åŸºäºstackingèåˆæ¨¡å‹çš„è€å¹´äººå¥åº·çŠ¶å†µé¢„æµ‹â€”ä»£ç éƒ¨åˆ†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2775adc",
   "metadata": {},
   "source": [
    "# 1.æ•°æ®é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc926539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¯¼å…¥æ‰€éœ€åŒ…\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import re as re\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "\n",
    "import time\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "%matplotlib inline\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¯¼å…¥æ•°æ®\n",
    "train             = pd.read_csv('train.csv')\n",
    "test              = pd.read_csv('test.csv')\n",
    "greeks            = pd.read_csv('greeks.csv')\n",
    "submission_df = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff39c9e",
   "metadata": {},
   "source": [
    "### 1.1æ•°æ®å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be138bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ç‰¹å¾æ•°é‡\n",
    "data = {\n",
    "    \"Feature\": [],\n",
    "    \"Counts\": []\n",
    "}\n",
    "\n",
    "for feature in train.columns:\n",
    "    if feature != \"null_count\":\n",
    "        data[\"Feature\"].append(feature)\n",
    "        data[\"Counts\"].append(train[feature].nunique())\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 10))\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "cmap = sns.color_palette(sns.light_palette(\"#6ecdff\", n_colors=len(train.columns), reverse=True))\n",
    "sns.set_palette(cmap)\n",
    "\n",
    "counts = pd.DataFrame(data)\n",
    "_ = sns.barplot(x=counts.Feature, y=counts.Counts, ax=ax, order=counts.sort_values('Counts', ascending=False).Feature)\n",
    "for p in ax.patches:\n",
    "    ax.text(x=p.get_x()+(p.get_width()/2), y=p.get_height(), s=\"{:,d}\".format(round(p.get_height())), ha=\"center\")\n",
    "_ = ax.set_title(\"Unique Values by Feature\", fontsize=15)\n",
    "_ = ax.set_ylabel(\"Number of Unique Values\", fontsize=15)\n",
    "_ = ax.set_xlabel(\"Feature\", fontsize=15)\n",
    "for label in ax.get_xticklabels():\n",
    "    label.set_rotation(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f49abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#åˆ†å¸ƒå›¾ï¼ˆå°æç´å›¾ï¼‰\n",
    "features = [\n",
    "    'AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', 'BC', 'BD ', 'BN',\n",
    "    'BP', 'BQ', 'BR', 'BZ', 'CB', 'CC', 'CD ', 'CF', 'CH', 'CL', 'CR', 'CS',\n",
    "    'CU', 'CW ', 'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', 'DU', 'DV', 'DY',\n",
    "    'EB', 'EE', 'EG', 'EH', 'EL', 'EP', 'EU', 'FC', 'FD ', 'FE', 'FI',\n",
    "    'FL', 'FR', 'FS', 'GB', 'GE', 'GF', 'GH', 'GI', 'GL'\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=11, ncols=5, figsize=(20, 30))\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "axis_counter = 0\n",
    "for feature in features:\n",
    "    ax = axs[axis_counter]\n",
    "    _ = sns.violinplot(y=train[feature], ax=ax)\n",
    "    _ = ax.set_title(\"{}\".format(feature))\n",
    "    _ = ax.set_ylabel(\"\")\n",
    "    _ = ax.set_xlabel(\"\")\n",
    "    axis_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d653f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#é¢„æµ‹Classçš„ç±»åˆ«\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 8))\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "cmap = sns.color_palette(sns.light_palette(\"#6495ED\", n_colors=8, reverse=True))\n",
    "sns.set_palette(cmap)\n",
    "counts = pd.DataFrame(greeks[\"Alpha\"].value_counts())\n",
    "_ = sns.barplot(x=counts.index, y=counts.Alpha, ax=axs[0])\n",
    "for p in axs[0].patches:\n",
    "    axs[0].text(x=p.get_x()+(p.get_width()/2), y=p.get_height(), s=\"{:,d}\".format(round(p.get_height())), ha=\"center\")\n",
    "_ = axs[0].set_title(\"Class Balance (Supplemental)\", fontsize=15)\n",
    "_ = axs[0].set_ylabel(\"Number of Records\", fontsize=15)\n",
    "_ = axs[0].set_xlabel(\"Age-Related Condition\", fontsize=15)\n",
    "\n",
    "targets = greeks[\"Alpha\"].unique()\n",
    "data = [greeks[(greeks[\"Alpha\"] == target)][\"Id\"].count() for target in targets]\n",
    "cmap = sns.color_palette(sns.light_palette(\"#6495ED\", n_colors=8, reverse=True))\n",
    "_ = axs[1].pie(\n",
    "    data, labels=targets,\n",
    "    autopct=lambda x: \"{:,.0f} = {:.2f}%\".format(x * sum(data)/100, x),\n",
    "    explode=[0.20] * len(data), \n",
    "    colors=cmap,\n",
    ")\n",
    "_ = axs[1].set_title(\"Class Balance (Supplemental)\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ecac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainæ•°æ®é›†çš„çŸ©é˜µçƒ­å›¾\n",
    "features = [\n",
    "    'AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', 'BC', 'BD ', 'BN',\n",
    "    'BP', 'BQ', 'BR', 'BZ', 'CB', 'CC', 'CD ', 'CF', 'CH', 'CL', 'CR', 'CS',\n",
    "    'CU', 'CW ', 'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', 'DU', 'DV', 'DY',\n",
    "    'EB', 'EE', 'EG', 'EH', 'EJ', 'EL', 'EP', 'EU', 'FC', 'FD ', 'FE', 'FI',\n",
    "    'FL', 'FR', 'FS', 'GB', 'GE', 'GF', 'GH', 'GI', 'GL', 'Class'\n",
    "]\n",
    "\n",
    "correlation_matrix = train[features].corr(method=\"spearman\")\n",
    "\n",
    "from matplotlib.colors import SymLogNorm\n",
    "\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "_ = sns.heatmap(\n",
    "    correlation_matrix, \n",
    "    mask=np.triu(np.ones_like(correlation_matrix, dtype=bool)), \n",
    "    cmap=sns.diverging_palette(220, 20, l=60, sep=10, as_cmap=True), \n",
    "    center=0,\n",
    "    square=True, \n",
    "    linewidths=.1, \n",
    "    cbar=False,\n",
    "    ax=ax,\n",
    "    annot=False,\n",
    ")\n",
    "_ = ax.set_title(\"Spearman Correlation Matrix\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cfbcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ•°æ®é›†ç¼ºå¤±å€¼æƒ…å†µ\n",
    "null_count_labels = [train[(train[\"null_count\"] == x)].isnull().sum().index[:-1] for x in [1, 2, 4]]\n",
    "null_count_values = [train[(train[\"null_count\"] == x)].isnull().sum().values[:-1] for x in [1, 2, 4]]\n",
    "null_count_numbers = [1, 2, 4]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(20, 15))\n",
    "fig.suptitle(\"Null Value Breakdown\", fontsize=20)\n",
    "\n",
    "axs = axs.flatten()\n",
    "axis_counter = 0\n",
    "\n",
    "for null_labels, null_values, null_numbers in zip(null_count_labels, null_count_values, null_count_numbers):\n",
    "    ax = axs[axis_counter]\n",
    "    _ = sns.barplot(x=null_labels, y=null_values, ax=ax)\n",
    "    _ = ax.set_title(\"Rows With {} Null(s)\".format(null_numbers), fontsize=15)\n",
    "    _ = ax.set_ylabel(\"\")\n",
    "    _ = ax.set_xlabel(\"\")\n",
    "    _ = ax.set_xticks([z for z in range(len(null_labels))], null_labels, rotation=90)\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(x=p.get_x()+(p.get_width()/2), y=height, s=\"{:d}\".format(int(height)), ha=\"center\")\n",
    "    axis_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ee0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ•°æ®é›†é‡å¤å€¼æƒ…å†µ\n",
    "duplicates = train.pivot_table(index=[\n",
    "    'AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', 'BC', 'BD ', 'BN',\n",
    "    'BP', 'BQ', 'BR', 'BZ', 'CB', 'CC', 'CD ', 'CF', 'CH', 'CL', 'CR', 'CS',\n",
    "    'CU', 'CW ', 'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', 'DU', 'DV', 'DY',\n",
    "    'EB', 'EE', 'EG', 'EH', 'EJ', 'EL', 'EP', 'EU', 'FC', 'FD ', 'FE', 'FI',\n",
    "    'FL', 'FR', 'FS', 'GB', 'GE', 'GF', 'GH', 'GI', 'GL'\n",
    "], aggfunc=\"size\")\n",
    "unique, counts = np.unique(duplicates, return_counts=True)\n",
    "value_counts = dict(zip(unique, counts))\n",
    "\n",
    "if len(unique) == 1:\n",
    "    print(\": There are no duplicated rows in the training set\")\n",
    "else:\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 8))\n",
    "\n",
    "    _ = sns.barplot(x=list(value_counts.keys())[1:], y=list(value_counts.values())[1:], ax=ax)\n",
    "    _ = ax.set_title(\"Duplicate Counts in Training Set\", fontsize=15)\n",
    "    _ = ax.set_ylabel(\"Count\")\n",
    "    _ = ax.set_xlabel(\"Number of Times Row is Duplicated\")\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(\n",
    "            x=p.get_x()+(p.get_width()/2),\n",
    "            y=height,\n",
    "            s=\"{:d}\".format(int(height)),\n",
    "            ha=\"center\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6311f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainæ•°æ®é›†æè¿°æ€§ç»Ÿè®¡\n",
    "features = [\n",
    "    'AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', 'BC', 'BD ', 'BN',\n",
    "    'BP', 'BQ', 'BR', 'BZ', 'CB', 'CC', 'CD ', 'CF', 'CH', 'CL', 'CR', 'CS',\n",
    "    'CU', 'CW ', 'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', 'DU', 'DV', 'DY',\n",
    "    'EB', 'EE', 'EG', 'EH', 'EJ', 'EL', 'EP', 'EU', 'FC', 'FD ', 'FE', 'FI',\n",
    "    'FL', 'FR', 'FS', 'GB', 'GE', 'GF', 'GH', 'GI', 'GL'\n",
    "]\n",
    "\n",
    "train[features].describe().T.style.bar(subset=['mean'], color='#7BCC70')\\\n",
    "    .background_gradient(subset=['std'], cmap='Reds')\\\n",
    "    .background_gradient(subset=['50%'], cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9965ca43",
   "metadata": {},
   "source": [
    "### 1.3æ•°æ®å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#å®šæ€§å˜é‡è½¬åŒ–\n",
    "train['EJ'] = train['EJ'].replace({'A': 0, 'B': 1})\n",
    "test['EJ'] = test['EJ'].replace({'A': 0, 'B': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdda006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ç”¨ä¸­ä½æ•°å¡«è¡¥ç¼ºå¤±å€¼\n",
    "train.fillna(train.median(),inplace=True)\n",
    "test.fillna(train.median(),inplace=True)\n",
    "test = test.drop(['Id'], axis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dca9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ•°æ®åˆ†å‰²\n",
    "x_train = train.drop(['Id','Class'], axis=True)\n",
    "y_train = train['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff68d57",
   "metadata": {},
   "source": [
    "# 2.åŸºäºOptunaä¼˜åŒ–æ¡†æ¶çš„å•æ¨¡å‹æ„å»º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657130d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¯¼å…¥æ‰€éœ€åŒ…\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostRegressor, CatBoostClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from skopt import BayesSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "import time\n",
    "from sklearn.metrics import log_loss\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64226bbc",
   "metadata": {},
   "source": [
    "## 2.1å®šä¹‰è¯„ä»·æŒ‡æ ‡balanced log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00628918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_logarithmic_loss_new(y_pred,y_true,):\n",
    "\n",
    "    # Nc is the number of observations\n",
    "    N_1 = np.sum(y_true == 1, axis=0)\n",
    "    N_0 = np.sum(y_true == 0, axis=0)\n",
    "\n",
    "    # In order to avoid the extremes of the log function, each predicted probability ğ‘ is replaced with max(min(ğ‘,1âˆ’10âˆ’15),10âˆ’15)\n",
    "    y_pred = np.maximum(np.minimum(y_pred, 1 - 1e-15), 1e-15)\n",
    "\n",
    "    # balanced logarithmic loss\n",
    "    loss_numerator = - (1/N_0) * np.sum((1 - y_true) * np.log(1-y_pred)) - (1/N_1) * np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "    return loss_numerator / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#åˆ’åˆ†å‡ºä¸€ä¸ªä¸ç”¨è®­ç»ƒï¼Œåªç”¨æ¥éªŒè¯çš„test\n",
    "x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(x_train, y_train, test_size=0.3, random_state=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b201aa3d",
   "metadata": {},
   "source": [
    "## 2.2 å‚æ•°ä¼˜åŒ–å’Œæ¨¡å‹å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71db621f",
   "metadata": {},
   "source": [
    "### 2.2.1 Optuna+LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2b1c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_losses_lightGBM = []\n",
    "time_lightGBM = []\n",
    "best_trial_value = []\n",
    "n_iterations = 10\n",
    "for i in range(n_iterations):\n",
    "    \n",
    "    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "    x_train2, x_test2, y_train2, y_test2 = train_test_split(x_train_1, y_train_1, test_size=0.3, random_state=i)#è®¾ç½®äº†éšæœºæ•°ç§å­\n",
    "    \n",
    "    \n",
    "   \n",
    "    def objective(trial):\n",
    "        param_grid = {\n",
    "            \"random_state\": 48,\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 1000, 30000),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 3000, step=20),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.01, 0.7),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.01, 0.7),\n",
    "            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.2, 0.95, step=0.1),\n",
    "            \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.2, 0.95, step=0.1),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 100, step=5),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 0.9)\n",
    "            #'categorical_feature': categorical_features\n",
    "        }\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)######random_state\n",
    "\n",
    "        scores = []\n",
    "        \n",
    "        #åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯åŠè¿›è¡Œå‚æ•°ä¼˜åŒ–\n",
    "        for train_index, test_index in skf.split(x_train2, y_train2):\n",
    "            #print(train_index)\n",
    "\n",
    "            x_train3, x_test3 = x_train2.iloc[train_index], x_train2.iloc[test_index]\n",
    "            y_train3, y_test3 = y_train2.iloc[train_index], y_train2.iloc[test_index]\n",
    "            \n",
    "\n",
    "            model = lgb.LGBMClassifier(**param_grid , class_weight='balanced')\n",
    "            model.fit(\n",
    "                x_train3,\n",
    "                y_train3,\n",
    "                eval_set=[(x_train3, y_train3),(x_test3, y_test3)],\n",
    "                early_stopping_rounds=300,\n",
    "                verbose=False,\n",
    "                categorical_feature=[39]\n",
    "            )\n",
    "            preds = model.predict_proba(x_test3)\n",
    "            #print(preds)\n",
    "            #print(preds[:, 1])\n",
    "            #print(y_test2)\n",
    "            score = balanced_logarithmic_loss_new(preds[:, 1],y_test3)\n",
    "            print(score)\n",
    "            scores.append(score)\n",
    "\n",
    "            \n",
    "        func_out = np.mean(scores)\n",
    "\n",
    "        return func_out\n",
    "\n",
    "    study = optuna.create_study(study_name=f'study{i}', direction=\"minimize\",storage='sqlite:///db.sqlite3')\n",
    "    study.optimize(objective, n_trials=200)\n",
    "    best_trial_value.append(study.best_trial.value)\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "    \n",
    "    \n",
    "    model = lgb.LGBMClassifier(**trial.params,class_weight='balanced')\n",
    "    \n",
    "    # è®°å½•å¼€å§‹æ—¶é—´\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.fit(x_train2, y_train2)\n",
    "    \n",
    "    # è®¡ç®—è®­ç»ƒæ—¶é—´\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    prediction = model.predict_proba(x_test2)[:, 1]\n",
    "    score_test = balanced_logarithmic_loss_new(prediction,y_test2)\n",
    "    \n",
    "    log_losses_lightGBM.append(score_test)\n",
    "    time_lightGBM.append(training_time)\n",
    "\n",
    "\n",
    "\n",
    "result_lightGBM = pd.DataFrame({'log_losses' : log_losses_lightGBM,'time_comsumed' : time_lightGBM,'best_value_lightGBM' : best_trial_value})\n",
    "result_lightGBM.to_csv('result_lightGBM.csv')\n",
    "\n",
    "'''\n",
    "Params: \n",
    "    bagging_fraction: 0.9\n",
    "    bagging_freq: 1\n",
    "    colsample_bytree: 0.8755453466152356\n",
    "    feature_fraction: 0.6000000000000001\n",
    "    learning_rate: 0.2941129864759795\n",
    "    max_depth: 4\n",
    "    min_child_samples: 85\n",
    "    n_estimators: 22976\n",
    "    num_leaves: 2870\n",
    "    reg_alpha: 0.515407087957732\n",
    "    reg_lambda: 0.26493493680587554\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02462a99",
   "metadata": {},
   "source": [
    "### 2.2.3 Optuna+CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a5209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "time_catboost = []\n",
    "log_losses_catboost = []\n",
    "best_trial_value = []\n",
    "n_iterations = 10\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "    x_train2, x_test2, y_train2, y_test2 = train_test_split(x_train_1, y_train_1, test_size=0.3, random_state=i)  # è®¾ç½®äº†éšæœºæ•°ç§å­\n",
    "\n",
    "    def objective(trial):\n",
    "        param_grid = {\n",
    "            \n",
    "            \"iterations\": trial.suggest_int(\"iterations\", 100, 1000),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 3, 12),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 0.01, 1.0),\n",
    "            \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 10.0),\n",
    "            \"random_strength\": trial.suggest_float(\"random_strength\", 0.0, 10.0),\n",
    "            \"border_count\": trial.suggest_int(\"border_count\", 1, 255),\n",
    "            \"scale_pos_weight\": sum(y_train1 == 0) / sum(y_train1 == 1),\n",
    "            \"use_best_model\": True,\n",
    "            \"random_seed\": 42,\n",
    "            \"logging_level\": \"Silent\",\n",
    "            \n",
    "        }\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        scores = []\n",
    "        # åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯åŠè¿›è¡Œå‚æ•°ä¼˜åŒ–\n",
    "        for train_index, test_index in skf.split(x_train2, y_train2):\n",
    "            x_train3, x_test3 = x_train2.iloc[train_index], x_train2.iloc[test_index]\n",
    "            y_train3, y_test3 = y_train2.iloc[train_index], y_train2.iloc[test_index]\n",
    "\n",
    "            model = CatBoostClassifier(**param_grid)\n",
    "            model.fit(\n",
    "                x_train3,\n",
    "                y_train3,\n",
    "                eval_set=(x_test3, y_test3),\n",
    "                early_stopping_rounds=200,\n",
    "            )\n",
    "            preds = model.predict_proba(x_test3)[:, 1]\n",
    "            score = balanced_logarithmic_loss_new(preds, y_test3)\n",
    "            print(score)\n",
    "            scores.append(score)\n",
    "\n",
    "        func_out = np.mean(scores)\n",
    "\n",
    "        return func_out\n",
    "\n",
    "    study = optuna.create_study(study_name=f'study{i+1}', direction=\"minimize\",storage='sqlite:///db.sqlite3')\n",
    "    study.optimize(objective, n_trials=200)\n",
    "    \n",
    "    best_trial_value.append(study.best_trial.value)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    #print(\"Number of finished trials: \", len(study.trials))\n",
    "    #print(\"Best trial:\")\n",
    "    #trial = study.best_trial\n",
    "\n",
    "    #print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in best_params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "  \n",
    "    # è®°å½•å¼€å§‹æ—¶é—´\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = CatBoostClassifier(**best_params,auto_class_weights='Balanced')\n",
    "    model.fit(x_train2, y_train2)\n",
    "    \n",
    "    # è®¡ç®—è®­ç»ƒæ—¶é—´\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    prediction = model.predict_proba(x_test2)[:, 1]\n",
    "    score_test = balanced_logarithmic_loss_new(prediction, y_test2)\n",
    "\n",
    "    log_losses_catboost.append(score_test)\n",
    "    time_catboost.append(training_time)\n",
    "\n",
    "\n",
    "\n",
    "result_catboost = pd.DataFrame({'log_losses_catboost' : log_losses_catboost,'time_comsumed_catboost' : time_catboost,'best_value_catboost' : best_trial_value})\n",
    "result_catboost.to_csv('result_catboost1.csv')\n",
    "\n",
    "'''\n",
    "Params: \n",
    "    bagging_temperature: 7.442784574066854\n",
    "    border_count: 212\n",
    "    depth: 3\n",
    "    iterations: 689\n",
    "    l2_leaf_reg: 0.8262450118748192\n",
    "    learning_rate: 0.09468235278046022\n",
    "    random_strength: 3.098327157242888\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b613e624",
   "metadata": {},
   "source": [
    "### 2.2.4 Optuna+éšæœºæ£®æ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48155ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "log_losses_rf = []\n",
    "best_trial_value = []\n",
    "time_rf = []\n",
    "n_iterations = 10\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "    x_train2, x_test2, y_train2, y_test2 = train_test_split(x_train_1, y_train_1, test_size=0.3, random_state=i)  # è®¾ç½®äº†éšæœºæ•°ç§å­\n",
    "\n",
    "    def objective(trial):\n",
    "        param_grid = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "            \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 5),\n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"]),\n",
    "            \"class_weight\": \"balanced\",\n",
    "            \"random_state\": 48,\n",
    "        }\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        scores = []\n",
    "        # åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯åŠè¿›è¡Œå‚æ•°ä¼˜åŒ–\n",
    "        for train_index, test_index in skf.split(x_train2, y_train2):\n",
    "            x_train3, x_test3 = x_train2.iloc[train_index], x_train2.iloc[test_index]\n",
    "            y_train3, y_test3 = y_train2.iloc[train_index], y_train2.iloc[test_index]\n",
    "\n",
    "            model = RandomForestClassifier(**param_grid)\n",
    "            model.fit(x_train3, y_train3)\n",
    "            preds = model.predict_proba(x_test3)[:, 1]\n",
    "            score = balanced_logarithmic_loss_new(preds, y_test3)\n",
    "            print(score)\n",
    "            scores.append(score)\n",
    "\n",
    "        func_out = np.mean(scores)\n",
    "\n",
    "        return func_out\n",
    "    #study = optuna.create_study(study_name = f'study{i+4}', direction=\"minimize\", storage='sqlite:///db.sqlite3')\n",
    "    study = optuna.create_study( direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    \n",
    "    best_trial_value.append(study.best_trial.value)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    print(\"  Params: \")\n",
    "    for key, value in best_params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "        \n",
    "    # è®°å½•å¼€å§‹æ—¶é—´\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = RandomForestClassifier(**best_params)\n",
    "    model.fit(x_train2, y_train2)\n",
    "    \n",
    "    # è®¡ç®—è®­ç»ƒæ—¶é—´\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    prediction = model.predict_proba(x_test2)[:, 1]\n",
    "    score_test = balanced_logarithmic_loss_new(prediction, y_test2)\n",
    "\n",
    "    log_losses_rf.append(score_test)\n",
    "    time_rf.append(training_time)\n",
    "\n",
    "\n",
    "result_rf = pd.DataFrame({'log_losses_rf' : log_losses_rf,'time_comsumed_rf' : time_rf,'best_value_rf' : best_trial_value})\n",
    "result_rf.to_csv('result_rf.csv')\n",
    "'''\n",
    "Params: \n",
    "    n_estimators: 190\n",
    "    criterion: entropy\n",
    "    max_depth: 5\n",
    "    min_samples_split: 9\n",
    "    min_samples_leaf: 5\n",
    "    max_features: sqrt\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a623089",
   "metadata": {},
   "source": [
    "### 2.2.5 Optuna+SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb36d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_losses_svm = []\n",
    "time_svm = []\n",
    "best_trial_value =[]\n",
    "\n",
    "n_iterations = 10\n",
    "\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "    x_train2, x_test2, y_train2, y_test2 = train_test_split(x_train_1, y_train_1, test_size=0.3, random_state=i)  # è®¾ç½®äº†éšæœºæ•°ç§å­\n",
    "\n",
    "\n",
    "    def objective(trial):\n",
    "        \n",
    "        \n",
    "        param_grid = {\n",
    "            \"C\": trial.suggest_loguniform(\"C\", 1e-3, 1e3),\n",
    "            \"kernel\": trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\", \"sigmoid\",'poly']),\n",
    "            \"gamma\": trial.suggest_categorical(\"gamma\", [\"scale\", \"auto\"]),\n",
    "            'coef0':0,\n",
    "            \"degree\": trial.suggest_int(\"degree\", 1, 5),\n",
    "            \"class_weight\": \"balanced\",\n",
    "            \"random_state\": 48,\n",
    "        }\n",
    "        \n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        scores = []\n",
    "        # åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯åŠè¿›è¡Œå‚æ•°ä¼˜åŒ–\n",
    "        for train_index, test_index in skf.split(x_train2, y_train2):\n",
    "            x_train3, x_test3 = x_train2.iloc[train_index], x_train2.iloc[test_index]\n",
    "            y_train3, y_test3 = y_train2.iloc[train_index], y_train2.iloc[test_index]\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(x_train3)\n",
    "            x_train3 = scaler.transform(x_train3)\n",
    "            x_train3 = pd.DataFrame(data=x_train3)\n",
    "\n",
    "            model = SVC(**param_grid,probability=True)\n",
    "            model.fit(x_train3, y_train3)\n",
    "            preds = model.decision_function(x_test3)\n",
    "            score = balanced_logarithmic_loss_new(preds, y_test3)\n",
    "\n",
    "            scores.append(score)\n",
    "\n",
    "        func_out = np.mean(scores)\n",
    "\n",
    "        return func_out\n",
    "\n",
    "    study = optuna.create_study(study_name=f'study-svc-{i}',direction=\"minimize\",storage='sqlite:///db.sqlite3')\n",
    "    study.optimize(objective, n_trials=200)\n",
    "    best_trial_value.append(study.best_trial.value)\n",
    "\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # è®°å½•å¼€å§‹æ—¶é—´\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = SVC(**best_params, probability=True)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(x_train2)\n",
    "    x_train1 = scaler.transform(x_train2)\n",
    "    x_train1 = pd.DataFrame(data=x_train2)\n",
    "\n",
    "    model.fit(x_train2, y_train2)\n",
    "\n",
    "    # è®¡ç®—è®­ç»ƒæ—¶é—´\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    prediction = model.predict_proba(x_test2)[:, 1]\n",
    "    score_test = balanced_logarithmic_loss_new(prediction, y_test2)\n",
    "\n",
    "    log_losses_svm.append(score_test)\n",
    "    time_svm.append(training_time)\n",
    "\n",
    "result_svm = pd.DataFrame({'log_losses' : log_losses_svm,\n",
    "                                     'time_comsumed' : time_svm, 'best_value' : best_trial_value})\n",
    "result_svm.to_csv('result_svm_10itrations0608.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e147a9ec",
   "metadata": {},
   "source": [
    "### 2.2.6 Optuna+XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a71ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_losses_xgboost = []\n",
    "time_xgboost = []\n",
    "best_trial_value =[]\n",
    "n_iterations = 10\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "    x_train2, x_test2, y_train2, y_test2 = train_test_split(x_train_1, y_train_1, test_size=0.3, random_state=i)  # è®¾ç½®äº†éšæœºæ•°ç§å­\n",
    "\n",
    "    def objective(trial):\n",
    "        param_grid = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            # \"eval_metric\": \"logloss\",\n",
    "            \"seed\": 48,\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 1000, 30000),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.01, 0.7),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1, step=0.1),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 0.9),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.01, 0.7),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.01, 0.7),\n",
    "            \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 0.1, 10),\n",
    "            'max_delta_step':trial.suggest_int(\"max_delta_step\", 0, 10),\n",
    "            'alpha':trial.suggest_float(\"alpha\", 0.01, 0.7)\n",
    "        }\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        scores = []\n",
    "        # åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯åŠè¿›è¡Œå‚æ•°ä¼˜åŒ–\n",
    "        for train_index, test_index in skf.split(x_train2, y_train2):\n",
    "            x_train3, x_test3 = x_train2.iloc[train_index], x_train2.iloc[test_index]\n",
    "            y_train3, y_test3 = y_train2.iloc[train_index], y_train2.iloc[test_index]\n",
    "\n",
    "\n",
    "            model = xgb.XGBClassifier(**param_grid,feval = balanced_logarithmic_loss_new)\n",
    "            model.fit(\n",
    "                x_train3,\n",
    "                y_train3,\n",
    "                eval_set=[(x_test3, y_test3)],\n",
    "                early_stopping_rounds=200,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "\n",
    "            preds = model.predict_proba(x_test3)[:, 1]\n",
    "            score = balanced_logarithmic_loss_new(preds, y_test3)\n",
    "            scores.append(score)\n",
    "\n",
    "        func_out = np.mean(scores)\n",
    "\n",
    "        return func_out\n",
    "\n",
    "    study = optuna.create_study(study_name=f'study{i}',direction=\"minimize\",storage='sqlite:///db.sqlite3')\n",
    "    study.optimize(objective, n_trials=200)\n",
    "    best_trial_value.append(study.best_trial.value)\n",
    "\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # è®°å½•å¼€å§‹æ—¶é—´\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = xgb.XGBClassifier(**best_params, feval = balanced_logarithmic_loss_new)\n",
    "    model.fit(x_train2, y_train2)\n",
    "\n",
    "    # è®¡ç®—è®­ç»ƒæ—¶é—´\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    prediction = model.predict_proba(x_test2)[:, 1]\n",
    "    score_test = balanced_logarithmic_loss_new(prediction, y_test2)\n",
    "\n",
    "    log_losses_xgboost.append(score_test)\n",
    "    time_xgboost.append(training_time)\n",
    "\n",
    "# log_losses_xgboost = pd.DataFrame(data=log_losses_xgboost)\n",
    "result_xgboost = pd.DataFrame({'log_losses' : log_losses_xgboost,\n",
    "                                     'time_comsumed' : time_xgboost, 'best_value' : best_trial_value})\n",
    "result_xgboost.to_csv('result_xgboost_10itrations0607.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eb233d",
   "metadata": {},
   "source": [
    "## 3.3 LightGBMæ¨¡å‹æ•ˆæœåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23af4726",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestparams_lgbm = {\n",
    "            'bagging_fraction': 0.9,\n",
    "            'bagging_freq': 1,\n",
    "            'colsample_bytree': 0.8755453466152356,\n",
    "            'feature_fraction': 0.6000000000000001,\n",
    "            'learning_rate': 0.2941129864759795,\n",
    "            'max_depth': 4,\n",
    "            'min_child_samples': 85,\n",
    "            'n_estimators': 22976,\n",
    "            'num_leaves': 2870,\n",
    "            'reg_alpha': 0.515407087957732,\n",
    "            'reg_lambda': 0.26493493680587554\n",
    "        }\n",
    "\n",
    "clf = lgb.LGBMClassifier(**bestparams_lgbm, class_weight='balanced')\n",
    "\n",
    "clf.fit(x_train_1, y_train_1,categorical_feature=[39])\n",
    "\n",
    "prediction = clf.predict_proba(x_test_1)[:, 1]\n",
    "score_test = balanced_logarithmic_loss_new(prediction,y_test_1)\n",
    "print('Test Score:', score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c491428",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestparams_lgbm = {\n",
    "            'bagging_fraction': 0.9,\n",
    "            'bagging_freq': 1,\n",
    "            'colsample_bytree': 0.8755453466152356,\n",
    "            'feature_fraction': 0.6000000000000001,\n",
    "            'learning_rate': 0.2941129864759795,\n",
    "            'max_depth': 4,\n",
    "            'min_child_samples': 85,\n",
    "            'n_estimators': 22976,\n",
    "            'num_leaves': 2870,\n",
    "            'reg_alpha': 0.515407087957732,\n",
    "            'reg_lambda': 0.26493493680587554\n",
    "        }\n",
    "\n",
    "clf = lgb.LGBMClassifier(**bestparams_lgbm, class_weight='balanced')\n",
    "\n",
    "clf.fit(x_train_1, y_train_1,categorical_feature=[39])\n",
    "\n",
    "prediction = clf.predict_proba(x_test_1)[:, 1]\n",
    "score_test = balanced_logarithmic_loss_new(prediction,y_test_1)\n",
    "print('Test Score:', score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fbcf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAP\n",
    "#å®˜ç½‘https://shap.readthedocs.io/en/latest/overviews.html\n",
    "#2ç¯‡è¾ƒå¥½çš„å¸–å­https://zhuanlan.zhihu.com/p/83412330, https://zhuanlan.zhihu.com/p/103370775\n",
    "\n",
    "#éœ€è¦å…ˆå®‰è£…shap: pip install shap\n",
    "import shap\n",
    "shap.initjs()  # notebookç¯å¢ƒä¸‹ï¼ŒåŠ è½½ç”¨äºå¯è§†åŒ–çš„JSä»£ç \n",
    "\n",
    "#åœ¨SHAPä¸­è¿›è¡Œæ¨¡å‹è§£é‡Šéœ€è¦å…ˆåˆ›å»ºä¸€ä¸ªexplainer\n",
    "#SHAPæ”¯æŒå¾ˆå¤šç±»å‹çš„explainer(ä¾‹å¦‚deep, gradient, kernel, linear, tree, sampling)\n",
    "explainer = shap.TreeExplainer(clf,model_output='raw')\n",
    "shap_values = explainer.shap_values(x_train_1) # ä¼ å…¥ç‰¹å¾çŸ©é˜µï¼Œè®¡ç®—SHAPå€¼\n",
    "y_base = explainer.expected_value\n",
    "print(y_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67366b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAPç‰¹å¾é‡è¦æ€§\n",
    "shap.summary_plot(shap_values[1], x_train_1, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f0f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å˜é‡é‡è¦æ€§ PDP\n",
    "#ç”±sklearnæ‰€å¾—çš„åä¾èµ–å›¾\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "#Outlook_Overcastå’ŒHumidityå¯¹P(Play=yes)çš„å½±å“\n",
    "disp1 = PartialDependenceDisplay.from_estimator(clf, x_train, features=[\"DA\",\"BQ\"],method=\"brute\") #featureï¼šé€‰ç”¨å“ªäº›å˜é‡\n",
    "#Outlook_Overcastå’ŒHumidityçš„äº¤äº’æ•ˆåº”å¯¹P(Play=yes)çš„å½±å“\n",
    "disp2 = PartialDependenceDisplay.from_estimator(clf, x_train, features=[(\"DA\",\"BQ\")], method=\"brute\") #ç”¨ï¼ˆï¼‰äºŒç»´åŒæ—¶å±•ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c6dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¯¹ç¬¬1ä¸ªæ ·æœ¬è§‚æµ‹ä½œforce plotâ€”â€”è€ƒè™‘f(x)=P(Y=1)\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][0,:], feature_names=x_train_1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e311a428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ‰€æœ‰æ ·æœ¬è§‚æµ‹çš„force plotâ€”â€”è€ƒè™‘f(x)=P(Y=1)\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], x_train_1.loc[:,['DU','BQ','AB']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476bb08d",
   "metadata": {},
   "source": [
    "# 3.åŸºäºStackingèåˆæ¨¡å‹æ„å»º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a528a7",
   "metadata": {},
   "source": [
    "## 3.1å•æ¨¡å‹é¢„æµ‹åŠé¢„æµ‹å€¼ç›¸å…³æ€§æ¢ç©¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d4d258",
   "metadata": {},
   "source": [
    "### 3.1.1 éšæœºæ£®æ—+è¾“å‡ºé¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342851ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "    x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(x_train_1, y_train_1, test_size=0.3, random_state=i)  # è®¾ç½®äº†éšæœºæ•°ç§å­\n",
    "\n",
    "    def objective(trial):\n",
    "        param_grid = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "            \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 5),\n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"]),\n",
    "            \"class_weight\": \"balanced\",\n",
    "            \"random_state\": 48,\n",
    "        }\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        scores = []\n",
    "        # åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯åŠè¿›è¡Œå‚æ•°ä¼˜åŒ–\n",
    "        for train_index, test_index in skf.split(x_train_2, y_train_2):\n",
    "            x_train_3, x_test_3 = x_train_2.iloc[train_index], x_train_2.iloc[test_index]\n",
    "            y_train_3, y_test_3 = y_train_2.iloc[train_index], y_train_2.iloc[test_index]\n",
    "\n",
    "            model = RandomForestClassifier(**param_grid)\n",
    "            model.fit(x_train_3, y_train_3)\n",
    "            preds = model.predict_proba(x_test_3)[:, 1]\n",
    "            score = balanced_logarithmic_loss_new(preds, y_test_3)\n",
    "            print(score)\n",
    "            scores.append(score)\n",
    "\n",
    "        func_out = np.mean(scores)\n",
    "\n",
    "        return func_out\n",
    "    \n",
    "    #study = optuna.create_study(study_name = f'study{i+4}', direction=\"minimize\", storage='sqlite:///db.sqlite3')\n",
    "    \n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=200)\n",
    "    \n",
    "    #best_trial_value.append(study.best_trial.value)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    print(\"  Params: \")\n",
    "    for key, value in best_params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "        \n",
    "    # è®°å½•å¼€å§‹æ—¶é—´\n",
    "    #start_time = time.time()\n",
    "    \n",
    "    model = RandomForestClassifier(**best_params)\n",
    "    model.fit(x_train_2, y_train_2)\n",
    "    \n",
    "    # è®¡ç®—è®­ç»ƒæ—¶é—´\n",
    "    #training_time = time.time() - start_time\n",
    "\n",
    "    prediction = model.predict_proba(x_test_2)[:, 1]\n",
    "    #score_test = balanced_logarithmic_loss_new(prediction, y_test_2)\n",
    "    \n",
    "\n",
    "    #log_losses_rf.append(score_test)\n",
    "    #time_rf.append(training_time)\n",
    "prediction = pd.DataFrame(prediction)\n",
    "prediction.to_csv('predictiont_rf.csv')\n",
    "'''\n",
    "Params: \n",
    "    n_estimators: 749\n",
    "    criterion: gini\n",
    "    max_depth: 11\n",
    "    min_samples_split: 10\n",
    "    min_samples_leaf: 5\n",
    "    max_features: sqrt\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eacba2",
   "metadata": {},
   "source": [
    "### 3.1.2 catboost+è¾“å‡ºé¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576369ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "    x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(x_train_1, y_train_1, test_size=0.3, random_state=i)  # è®¾ç½®äº†éšæœºæ•°ç§å­\n",
    "\n",
    "    def objective(trial):\n",
    "        param_grid = {\n",
    "            \n",
    "            \"iterations\": trial.suggest_int(\"iterations\", 100, 1000),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 3, 12),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 0.01, 1.0),\n",
    "            \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 10.0),\n",
    "            \"random_strength\": trial.suggest_float(\"random_strength\", 0.0, 10.0),\n",
    "            \"border_count\": trial.suggest_int(\"border_count\", 1, 255),\n",
    "            \"scale_pos_weight\": sum(y_train_2 == 0) / sum(y_train_2 == 1),\n",
    "            \"use_best_model\": True,\n",
    "            \"random_seed\": 48,\n",
    "            \"logging_level\": \"Silent\"\n",
    "            \n",
    "        }\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        scores = []\n",
    "        # åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯åŠè¿›è¡Œå‚æ•°ä¼˜åŒ–\n",
    "        for train_index, test_index in skf.split(x_train_2, y_train_2):\n",
    "            x_train_3, x_test_3 = x_train_2.iloc[train_index], x_train_2.iloc[test_index]\n",
    "            y_train_3, y_test_3 = y_train_2.iloc[train_index], y_train_2.iloc[test_index]\n",
    "\n",
    "\n",
    "            model = CatBoostClassifier(**param_grid)\n",
    "            model.fit(\n",
    "                x_train_3,\n",
    "                y_train_3,\n",
    "                eval_set=(x_test_3, y_test_3),\n",
    "                early_stopping_rounds=200,\n",
    "            )\n",
    "            preds = model.predict_proba(x_test_3)[:, 1]\n",
    "            score = balanced_logarithmic_loss_new(preds, y_test_3)\n",
    "            #print(score)\n",
    "            scores.append(score)\n",
    "\n",
    "        func_out = np.mean(scores)\n",
    "\n",
    "        return func_out\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=200)\n",
    "    \n",
    "    #best_trial_value.append(study.best_trial.value)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    #print(\"Number of finished trials: \", len(study.trials))\n",
    "    #print(\"Best trial:\")\n",
    "    #trial = study.best_trial\n",
    "\n",
    "    #print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in best_params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "  \n",
    "    # è®°å½•å¼€å§‹æ—¶é—´\n",
    "    #start_time = time.time()\n",
    "    \n",
    "    model = CatBoostClassifier(**best_params,auto_class_weights='Balanced')\n",
    "    model.fit(x_train_2, y_train_2)\n",
    "    \n",
    "    # è®¡ç®—è®­ç»ƒæ—¶é—´\n",
    "    #training_time = time.time() - start_time\n",
    "\n",
    "    prediction = model.predict_proba(x_test_2)[:, 1]\n",
    "    score_test = balanced_logarithmic_loss_new(prediction, y_test_2)\n",
    "\n",
    "    #log_losses_catboost.append(score_test)\n",
    "    #time_catboost.append(training_time)\n",
    "\n",
    "prediction = pd.DataFrame(prediction)\n",
    "prediction.to_csv('predictiont_catboost.csv')\n",
    "'''\n",
    "Params: \n",
    "    iterations: 852\n",
    "    learning_rate: 0.1266693440048803\n",
    "    depth: 3\n",
    "    l2_leaf_reg: 0.7364685775851822\n",
    "    bagging_temperature: 5.05484109041439\n",
    "    random_strength: 0.4104855929305867\n",
    "    border_count: 177\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30230ace",
   "metadata": {},
   "source": [
    "### 3.1.3 lightgbm+è¾“å‡ºé¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e1452",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1\n",
    "for i in range(n_iterations):\n",
    "    \n",
    "    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "    x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(x_train_1, y_train_1, test_size=0.3, random_state=i)#è®¾ç½®äº†éšæœºæ•°ç§å­\n",
    "    \n",
    "    \n",
    "    #x_test1_2 = lgb.Dataset(data=x_test1_1,label=y_test1,categorical_feature=['EJ'],free_raw_data=False)\n",
    "    \n",
    "       \n",
    "    def objective(trial):\n",
    "        \n",
    "        param_grid = {\n",
    "            \"random_state\": 48,\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 1000, 30000),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 3000, step=20),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.01, 0.7),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.01, 0.7),\n",
    "            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.2, 0.95, step=0.1),\n",
    "            \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.2, 0.95, step=0.1),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 100, step=5),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 0.9)\n",
    "            #'categorical_feature': categorical_features\n",
    "        }\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        scores = []\n",
    "        \n",
    "        #åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯åŠè¿›è¡Œå‚æ•°ä¼˜åŒ–\n",
    "        for train_index, test_index in skf.split(x_train_2, y_train_2):\n",
    "            #print(train_index)\n",
    "\n",
    "            x_train_3, x_test_3 = x_train_2.iloc[train_index], x_train_2.iloc[test_index]\n",
    "            y_train_3, y_test_3 = y_train_2.iloc[train_index], y_train_2.iloc[test_index]\n",
    "            \n",
    "\n",
    "            model = lgb.LGBMClassifier(**param_grid , class_weight='balanced')\n",
    "            model.fit(\n",
    "                x_train_3,\n",
    "                y_train_3,\n",
    "                eval_set=[(x_train_3, y_train_3),(x_test_3, y_test_3)],\n",
    "                early_stopping_rounds=200,\n",
    "                verbose=False,\n",
    "                categorical_feature=[39]\n",
    "            )\n",
    "            preds = model.predict_proba(x_test_3)\n",
    "            #print(preds)\n",
    "            #print(preds[:, 1])\n",
    "            #print(y_test2)\n",
    "            score = balanced_logarithmic_loss_new(preds[:, 1],y_test_3)\n",
    "            #print(score)\n",
    "            scores.append(score)\n",
    "\n",
    "            \n",
    "        func_out = np.mean(scores)\n",
    "\n",
    "        return func_out\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=200)\n",
    "    \n",
    "    #best_trial_value.append(study.best_trial.value)\n",
    "    #print(\"Number of finished trials: \", len(study.trials))\n",
    "    #print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    #print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "    \n",
    "    \n",
    "    model = lgb.LGBMClassifier(**trial.params,class_weight='balanced')\n",
    "    \n",
    "    # è®°å½•å¼€å§‹æ—¶é—´\n",
    "    #start_time = time.time()\n",
    "    \n",
    "    model.fit(x_train_2, y_train_2)\n",
    "    \n",
    "    # è®¡ç®—è®­ç»ƒæ—¶é—´\n",
    "    #training_time = time.time() - start_time\n",
    "    \n",
    "    prediction = model.predict_proba(x_test_2)[:, 1]\n",
    "    #score_test = balanced_logarithmic_loss_new(prediction,y_test_2)\n",
    "\n",
    "prediction = pd.DataFrame(prediction)\n",
    "prediction.to_csv('predictiont_lightgbm.csv')\n",
    "'''\n",
    "Params: \n",
    "    n_estimators: 26732\n",
    "    learning_rate: 0.1769023557198449\n",
    "    num_leaves: 2690\n",
    "    max_depth: 12\n",
    "    reg_alpha: 0.1833787704392688\n",
    "    reg_lambda: 0.06989518570851735\n",
    "    bagging_fraction: 0.9\n",
    "    bagging_freq: 1\n",
    "    feature_fraction: 0.9\n",
    "    min_child_samples: 60\n",
    "    colsample_bytree: 0.5391855748943887\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649b138f",
   "metadata": {},
   "source": [
    "### 3.1.4 XGBoost+è¾“å‡ºé¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acafcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "    x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(x_train_1, y_train_1, test_size=0.3, random_state=i)\n",
    "\n",
    "    def objective(trial):\n",
    "        param_grid = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            # \"eval_metric\": \"logloss\",\n",
    "            \"seed\": 48,\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 1000, 30000),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.01, 0.7),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1, step=0.1),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 0.9),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.01, 0.7),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.01, 0.7),\n",
    "            \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 0.1, 10),\n",
    "            'max_delta_step':trial.suggest_int(\"max_delta_step\", 0, 10),\n",
    "            'alpha':trial.suggest_float(\"alpha\", 0.01, 0.7)\n",
    "        }\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        scores = []\n",
    "        # åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯åŠè¿›è¡Œå‚æ•°ä¼˜åŒ–\n",
    "        for train_index, test_index in skf.split(x_train_2, y_train_2):\n",
    "            x_train_3, x_test_3 = x_train_2.iloc[train_index], x_train_2.iloc[test_index]\n",
    "            y_train_3, y_test_3 = y_train_2.iloc[train_index], y_train_2.iloc[test_index]\n",
    "\n",
    "            model = xgb.XGBClassifier(**param_grid,feval = balanced_logarithmic_loss_new)\n",
    "            model.fit(x_train_3, y_train_3,\n",
    "                      eval_set=[(x_test_3, y_test_3)],\n",
    "                      early_stopping_rounds=200,\n",
    "                      verbose=False)\n",
    "            preds = model.predict_proba(x_test_3)[:, 1]\n",
    "            score = balanced_logarithmic_loss_new(preds, y_test_3)\n",
    "\n",
    "            print(score)\n",
    "            scores.append(score)\n",
    "\n",
    "        func_out = np.mean(scores)\n",
    "\n",
    "        return func_out\n",
    "    \n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=200)\n",
    "\n",
    "\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    print(\"  Params: \")\n",
    "    for key, value in best_params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    \n",
    "    model = xgb.XGBClassifier(**best_params, feval = balanced_logarithmic_loss_new)\n",
    "    model.fit(x_train_2, y_train_2)\n",
    "\n",
    "\n",
    "    prediction = model.predict_proba(x_test_2)[:, 1]\n",
    "    score_test = balanced_logarithmic_loss_new(prediction, y_test_2)\n",
    "\n",
    "\n",
    "prediction = pd.DataFrame({'XBG':prediction})\n",
    "prediction.to_csv('prediction_XGB.csv')\n",
    "\n",
    "'''\n",
    "Best Params:\n",
    "n_estimators: 19808\n",
    "learning_rate: 0.29003009973815874\n",
    "max_depth: 11\n",
    "gamma: 0.3584743148039589\n",
    "subsample: 0.7\n",
    "colsample_bytree: 0.6874234399659787\n",
    "reg_alpha: 0.14303443256590267\n",
    "reg_lambda: 0.30450443282388806\n",
    "min_child_weight: 2.78622578591918\n",
    "max_delta_step: 6\n",
    "alpha: 0.43245457551855127\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07626d28",
   "metadata": {},
   "source": [
    "### 3.1.5 çƒ­åŠ›å›¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc76ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¢„æµ‹æ•ˆæœçƒ­åŠ›å›¾\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('prediction_model(1).csv')\n",
    "df.columns = ['XGB','CatBoost','LightGBM','RF']\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_context({\"figure.figsize\":(8,8)})\n",
    "sns.heatmap(data=df.corr(),square=True,annot=True,linewidths=0.8)\n",
    "plt.show(block = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a094218",
   "metadata": {},
   "source": [
    "## 3.2 Stackingèåˆæ¨¡å‹æ„å»º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7321ad",
   "metadata": {},
   "source": [
    "### 3.2.1 åŸºå­¦ä¹ å™¨ï¼šLGBM, CatBoost, XGBoost\tå…ƒå­¦ä¹ å™¨ï¼šLogisticå›å½’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb7099",
   "metadata": {},
   "outputs": [],
   "source": [
    "_N_FOLDS = 5\n",
    "_N_CLASS = 2\n",
    "kf = KFold(n_splits=_N_FOLDS, shuffle=True, random_state=68)\n",
    "\n",
    "def get_oof(clfname, X_train, y_train, X_test):\n",
    "    if clfname == 'xgb':\n",
    "        bestparams_xgb = {\n",
    "            'n_estimators': 19808,\n",
    "            'learning_rate': 0.29003009973815874,\n",
    "            'max_depth': 11,\n",
    "            'gamma': 0.3584743148039589,\n",
    "            'subsample': 0.7,\n",
    "            'colsample_bytree': 0.6874234399659787,\n",
    "            'reg_alpha': 0.14303443256590267,\n",
    "            'reg_lambda': 0.30450443282388806,\n",
    "            'min_child_weight': 2.78622578591918,\n",
    "            'max_delta_step': 6,\n",
    "            'alpha': 0.43245457551855127,\n",
    "        }\n",
    "        clf = xgb.XGBClassifier(**bestparams_xgb,feval = balanced_logarithmic_loss_new)\n",
    "\n",
    "    if clfname == 'cat':\n",
    "        bestparams_cat = {\n",
    "            'bagging_temperature': 7.442784574066854,\n",
    "            'border_count': 212,\n",
    "            'depth': 3,\n",
    "            'iterations': 689,\n",
    "            'l2_leaf_reg': 0.8262450118748192,\n",
    "            'learning_rate': 0.09468235278046022,\n",
    "            'random_strength': 3.098327157242888\n",
    "        }\n",
    "        clf = CatBoostClassifier(**bestparams_cat,auto_class_weights='Balanced')\n",
    "\n",
    "    if clfname == 'lgbm':\n",
    "        bestparams_lgbm = {\n",
    "            'bagging_fraction': 0.9,\n",
    "            'bagging_freq': 1,\n",
    "            'colsample_bytree': 0.8755453466152356,\n",
    "            'feature_fraction': 0.6000000000000001,\n",
    "            'learning_rate': 0.2941129864759795,\n",
    "            'max_depth': 4,\n",
    "            'min_child_samples': 85,\n",
    "            'n_estimators': 22976,\n",
    "            'num_leaves': 2870,\n",
    "            'reg_alpha': 0.515407087957732,\n",
    "            'reg_lambda': 0.26493493680587554\n",
    "        }\n",
    "        clf = lgb.LGBMClassifier(**bestparams_lgbm, class_weight='balanced')\n",
    "    # X_train: *\n",
    "    # y_train: 1 *\n",
    "    # X_test :  *\n",
    "    oof_train = np.zeros((X_train.shape[0], _N_CLASS))  # Stackingåè®­ç»ƒæ•°æ®çš„è¾“å‡º\n",
    "    oof_test = np.zeros((X_test.shape[0], _N_CLASS))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X_train)): # äº¤å‰éªŒè¯åˆ’åˆ†æ­¤æ—¶çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "        kf_X_train = X_train.iloc[train_index]  # è®­ç»ƒé›†\n",
    "        kf_y_train = y_train.iloc[train_index]  # è®­ç»ƒé›†å¯¹åº”çš„è¾“å‡º\n",
    "        kf_X_test = X_train.iloc[test_index]  # éªŒè¯é›†\n",
    "        kf_y_test = y_train.iloc[test_index]\n",
    "        if clfname == 'lgbm':\n",
    "            clf.fit(kf_X_train, kf_y_train,categorical_feature=[39],early_stopping_rounds=200,eval_set=[(kf_X_test, kf_y_test)])\n",
    "        else:\n",
    "            clf.fit(kf_X_train, kf_y_train,early_stopping_rounds=200,eval_set=[(kf_X_test, kf_y_test)])  # å½“å‰æ¨¡å‹è¿›è¡Œè®­ç»ƒ\n",
    "\n",
    "        oof_train[test_index] = clf.predict_proba(kf_X_test)# å¯¹å½“å‰éªŒè¯é›†è¿›è¡Œé¢„æµ‹\n",
    "        oof_test += np.around(clf.predict_proba(X_test),6) # å¯¹æµ‹è¯•é›†é¢„æµ‹\n",
    "\n",
    "    oof_test /= 5   # å¯¹æ¯ä¸€åˆ™äº¤å‰éªŒè¯çš„ç»“æœå–å¹³å‡\n",
    "    return oof_train, oof_test  # è¿”å›å½“å‰åˆ†ç±»å™¨å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„é¢„æµ‹ç»“æœ\n",
    "\n",
    "# å°†æ•°æ®æ¢æˆä½ çš„æ•°æ®\n",
    "X_train = x_train_1\n",
    "y_train = y_train_1\n",
    "X_test = x_test_1\n",
    "\n",
    "# å°†ä½ çš„æ¯ä¸ªåˆ†ç±»å™¨éƒ½è°ƒç”¨get_oofå‡½æ•°ï¼Œå¹¶æŠŠå®ƒä»¬çš„ç»“æœåˆå¹¶ï¼Œå°±å¾—åˆ°äº†æ–°çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®new_train,new_test\n",
    "new_train, new_test = [], []\n",
    "for clfname in ['lgbm','cat','xgb']:\n",
    "    oof_train, oof_test = get_oof(clfname, X_train, y_train, X_test)\n",
    "    new_train.append(oof_train)\n",
    "    new_test.append(oof_test)\n",
    "\n",
    "new_train = np.concatenate(new_train, axis=1)\n",
    "new_test = np.concatenate(new_test, axis=1)\n",
    "new_train = pd.DataFrame(new_train)\n",
    "new_test = pd.DataFrame(new_test)\n",
    "new_train.columns = ['lgbm0','lgbm1','cat0','cat1','xgb0','xgb1']\n",
    "new_test.columns = ['lgbm0','lgbm1','cat0','cat1','xgb0','xgb1']\n",
    "new_train = new_train[['lgbm1','cat1','xgb1']]\n",
    "new_test = new_test[['lgbm1','cat1','xgb1']]\n",
    "\n",
    "# ç”¨æ–°çš„è®­ç»ƒæ•°æ®new_trainä½œä¸ºæ–°çš„æ¨¡å‹çš„è¾“å…¥ï¼Œstackingç¬¬äºŒå±‚\n",
    "x_train4, y_train4 = new_train, y_train\n",
    "def objective(trial):\n",
    "    param_grid = {\n",
    "        \"C\": trial.suggest_float(\"C\", 1e-3, 1e2),\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", ['liblinear', 'saga']),\n",
    "        'penalty': trial.suggest_categorical('penalty', [\"l1\", \"l2\"])\n",
    "    }\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=72)\n",
    "    scores = []\n",
    "    # åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯åŠè¿›è¡Œå‚æ•°ä¼˜åŒ–\n",
    "    for train_index, test_index in skf.split(x_train4, y_train4):\n",
    "        x_train5, x_test5 = x_train4.iloc[train_index], x_train4.iloc[test_index]\n",
    "        y_train5, y_test5 = y_train4.iloc[train_index], y_train4.iloc[test_index]\n",
    "\n",
    "        model = LogisticRegression(**param_grid)\n",
    "        model.fit(x_train5, y_train5)\n",
    "        preds = model.predict_proba(x_test5)[:, 1]\n",
    "        score = balanced_logarithmic_loss_new(preds, y_test5)\n",
    "        print(score)\n",
    "        scores.append(score)\n",
    "\n",
    "    func_out = np.mean(scores)\n",
    "\n",
    "    return func_out\n",
    "study = optuna.create_study(study_name=f'stacking_study_lg',direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=200)\n",
    "best_params = study.best_params\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in best_params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "# è®°å½•å¼€å§‹æ—¶é—´\n",
    "start_time = time.time()\n",
    "\n",
    "stacking_model = LogisticRegression(**best_params)\n",
    "stacking_model.fit(new_train, y_train)\n",
    "\n",
    "# è®¡ç®—è®­ç»ƒæ—¶é—´\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "prediction = stacking_model.predict_proba(new_test)[:, 1]\n",
    "score_test = balanced_logarithmic_loss_new(prediction, y_test_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeaa4a0",
   "metadata": {},
   "source": [
    "### 3.2.2 åŸºå­¦ä¹ å™¨ï¼šLGBM, CatBoost, RF å…ƒå­¦ä¹ å™¨ï¼šLogisticå›å½’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba6e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_N_FOLDS = 5\n",
    "_N_CLASS = 2\n",
    "kf = KFold(n_splits=_N_FOLDS, shuffle=True, random_state=68)\n",
    "\n",
    "def get_oof(clfname, X_train, y_train, X_test):\n",
    "    if clfname == 'rf':\n",
    "        bestparams_rf = {\n",
    "            'n_estimators': 190,\n",
    "            'criterion': 'entropy',\n",
    "            'max_depth': 5,\n",
    "            'min_samples_split': 9,\n",
    "            'min_samples_leaf': 5,\n",
    "            'max_features': 'sqrt'\n",
    "        }\n",
    "        clf = RandomForestClassifier(**bestparams_rf)\n",
    "\n",
    "    if clfname == 'cat':\n",
    "        bestparams_cat = {\n",
    "            'bagging_temperature': 7.442784574066854,\n",
    "            'border_count': 212,\n",
    "            'depth': 3,\n",
    "            'iterations': 689,\n",
    "            'l2_leaf_reg': 0.8262450118748192,\n",
    "            'learning_rate': 0.09468235278046022,\n",
    "            'random_strength': 3.098327157242888\n",
    "        }\n",
    "        clf = CatBoostClassifier(**bestparams_cat,auto_class_weights='Balanced')\n",
    "\n",
    "    if clfname == 'lgbm':\n",
    "        bestparams_lgbm = {\n",
    "            'bagging_fraction': 0.9,\n",
    "            'bagging_freq': 1,\n",
    "            'colsample_bytree': 0.8755453466152356,\n",
    "            'feature_fraction': 0.6000000000000001,\n",
    "            'learning_rate': 0.2941129864759795,\n",
    "            'max_depth': 4,\n",
    "            'min_child_samples': 85,\n",
    "            'n_estimators': 22976,\n",
    "            'num_leaves': 2870,\n",
    "            'reg_alpha': 0.515407087957732,\n",
    "            'reg_lambda': 0.26493493680587554\n",
    "        }\n",
    "        clf = lgb.LGBMClassifier(**bestparams_lgbm, class_weight='balanced')\n",
    "    # X_train: *\n",
    "    # y_train: 1 *\n",
    "    # X_test :  *\n",
    "    oof_train = np.zeros((X_train.shape[0], _N_CLASS))  # Stackingåè®­ç»ƒæ•°æ®çš„è¾“å‡º\n",
    "    oof_test = np.zeros((X_test.shape[0], _N_CLASS))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X_train)): # äº¤å‰éªŒè¯åˆ’åˆ†æ­¤æ—¶çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "        kf_X_train = X_train.iloc[train_index]  # è®­ç»ƒé›†\n",
    "        kf_y_train = y_train.iloc[train_index]  # è®­ç»ƒé›†å¯¹åº”çš„è¾“å‡º\n",
    "        kf_X_test = X_train.iloc[test_index]  # éªŒè¯é›†\n",
    "        kf_y_test = y_train.iloc[test_index]\n",
    "        if clfname == 'lgbm':\n",
    "            clf.fit(kf_X_train, kf_y_train,categorical_feature=[39],early_stopping_rounds=200,eval_set=[(kf_X_test, kf_y_test)])\n",
    "        elif clfname == 'rf':\n",
    "            clf.fit(kf_X_train, kf_y_train)  # å½“å‰æ¨¡å‹è¿›è¡Œè®­ç»ƒ\n",
    "        else:\n",
    "            clf.fit(kf_X_train, kf_y_train, early_stopping_rounds=200, eval_set=[(kf_X_test, kf_y_test)])\n",
    "\n",
    "        oof_train[test_index] = clf.predict_proba(kf_X_test)# å¯¹å½“å‰éªŒè¯é›†è¿›è¡Œé¢„æµ‹\n",
    "        oof_test += np.around(clf.predict_proba(X_test),6) # å¯¹æµ‹è¯•é›†é¢„æµ‹\n",
    "\n",
    "    oof_test /= 5   # å¯¹æ¯ä¸€åˆ™äº¤å‰éªŒè¯çš„ç»“æœå–å¹³å‡\n",
    "    return oof_train, oof_test  # è¿”å›å½“å‰åˆ†ç±»å™¨å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„é¢„æµ‹ç»“æœ\n",
    "\n",
    "# å°†æ•°æ®æ¢æˆä½ çš„æ•°æ®\n",
    "X_train = x_train_1\n",
    "y_train = y_train_1\n",
    "X_test = x_test_1\n",
    "\n",
    "# å°†ä½ çš„æ¯ä¸ªåˆ†ç±»å™¨éƒ½è°ƒç”¨get_oofå‡½æ•°ï¼Œå¹¶æŠŠå®ƒä»¬çš„ç»“æœåˆå¹¶ï¼Œå°±å¾—åˆ°äº†æ–°çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®new_train,new_test\n",
    "new_train, new_test = [], []\n",
    "for clfname in ['lgbm','cat','rf']:\n",
    "    oof_train, oof_test = get_oof(clfname, X_train, y_train, X_test)\n",
    "    new_train.append(oof_train)\n",
    "    new_test.append(oof_test)\n",
    "\n",
    "new_train = np.concatenate(new_train, axis=1)\n",
    "new_test = np.concatenate(new_test, axis=1)\n",
    "new_train = pd.DataFrame(new_train)\n",
    "new_test = pd.DataFrame(new_test)\n",
    "new_train.columns = ['lgbm0','lgbm1','cat0','cat1','rf0','rf1']\n",
    "new_test.columns = ['lgbm0','lgbm1','cat0','cat1','rf0','rf1']\n",
    "new_train = new_train[['lgbm1','cat1','rf1']]\n",
    "new_test = new_test[['lgbm1','cat1','rf1']]\n",
    "\n",
    "# ç”¨æ–°çš„è®­ç»ƒæ•°æ®new_trainä½œä¸ºæ–°çš„æ¨¡å‹çš„è¾“å…¥ï¼Œstackingç¬¬äºŒå±‚\n",
    "x_train4, y_train4 = new_train, y_train\n",
    "def objective(trial):\n",
    "    param_grid = {\n",
    "        \"C\": trial.suggest_float(\"C\", 1e-3, 1e2),\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", ['liblinear', 'saga']),\n",
    "        'penalty': trial.suggest_categorical('penalty', [\"l1\", \"l2\"])\n",
    "    }\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=72)\n",
    "    scores = []\n",
    "    # åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯åŠè¿›è¡Œå‚æ•°ä¼˜åŒ–\n",
    "    for train_index, test_index in skf.split(x_train4, y_train4):\n",
    "        x_train5, x_test5 = x_train4.iloc[train_index], x_train4.iloc[test_index]\n",
    "        y_train5, y_test5 = y_train4.iloc[train_index], y_train4.iloc[test_index]\n",
    "\n",
    "        model = LogisticRegression(**param_grid)\n",
    "        model.fit(x_train5, y_train5)\n",
    "        preds = model.predict_proba(x_test5)[:, 1]\n",
    "        score = balanced_logarithmic_loss_new(preds, y_test5)\n",
    "        print(score)\n",
    "        scores.append(score)\n",
    "\n",
    "    func_out = np.mean(scores)\n",
    "\n",
    "    return func_out\n",
    "study = optuna.create_study(study_name=f'stacking_study_lg',direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=200)\n",
    "best_params = study.best_params\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in best_params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "# è®°å½•å¼€å§‹æ—¶é—´\n",
    "start_time = time.time()\n",
    "\n",
    "stacking_model = LogisticRegression(**best_params)\n",
    "stacking_model.fit(new_train, y_train)\n",
    "\n",
    "# è®¡ç®—è®­ç»ƒæ—¶é—´\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "prediction = stacking_model.predict_proba(new_test)[:, 1]\n",
    "score_test = balanced_logarithmic_loss_new(prediction, y_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ee0c64",
   "metadata": {},
   "source": [
    "### 3.2.3 åŸºå­¦ä¹ å™¨ï¼šLGBM, CatBoost, éšæœºæ£®æ— å…ƒå­¦ä¹ å™¨ï¼šéšæœºæ£®æ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b4fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_N_FOLDS = 5\n",
    "_N_CLASS = 2\n",
    "kf = KFold(n_splits=_N_FOLDS, shuffle=True, random_state=68)\n",
    "\n",
    "def get_oof(clfname, X_train, y_train, X_test):\n",
    "    if clfname == 'rf':\n",
    "        bestparams_rf = {\n",
    "            'n_estimators': 190,\n",
    "            'criterion': 'entropy',\n",
    "            'max_depth': 5,\n",
    "            'min_samples_split': 9,\n",
    "            'min_samples_leaf': 5,\n",
    "            'max_features': 'sqrt'\n",
    "        }\n",
    "        clf = RandomForestClassifier(**bestparams_rf)\n",
    "\n",
    "    if clfname == 'cat':\n",
    "        bestparams_cat = {\n",
    "            'bagging_temperature': 7.442784574066854,\n",
    "            'border_count': 212,\n",
    "            'depth': 3,\n",
    "            'iterations': 689,\n",
    "            'l2_leaf_reg': 0.8262450118748192,\n",
    "            'learning_rate': 0.09468235278046022,\n",
    "            'random_strength': 3.098327157242888\n",
    "        }\n",
    "        clf = CatBoostClassifier(**bestparams_cat,auto_class_weights='Balanced')\n",
    "\n",
    "    if clfname == 'lgbm':\n",
    "        bestparams_lgbm = {\n",
    "            'bagging_fraction': 0.9,\n",
    "            'bagging_freq': 1,\n",
    "            'colsample_bytree': 0.8755453466152356,\n",
    "            'feature_fraction': 0.6000000000000001,\n",
    "            'learning_rate': 0.2941129864759795,\n",
    "            'max_depth': 4,\n",
    "            'min_child_samples': 85,\n",
    "            'n_estimators': 22976,\n",
    "            'num_leaves': 2870,\n",
    "            'reg_alpha': 0.515407087957732,\n",
    "            'reg_lambda': 0.26493493680587554\n",
    "        }\n",
    "        clf = lgb.LGBMClassifier(**bestparams_lgbm, class_weight='balanced')\n",
    "    # X_train: *\n",
    "    # y_train: 1 *\n",
    "    # X_test :  *\n",
    "    oof_train = np.zeros((X_train.shape[0], _N_CLASS))  # Stackingåè®­ç»ƒæ•°æ®çš„è¾“å‡º\n",
    "    oof_test = np.zeros((X_test.shape[0], _N_CLASS))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X_train)): # äº¤å‰éªŒè¯åˆ’åˆ†æ­¤æ—¶çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "        kf_X_train = X_train.iloc[train_index]  # è®­ç»ƒé›†\n",
    "        kf_y_train = y_train.iloc[train_index]  # è®­ç»ƒé›†å¯¹åº”çš„è¾“å‡º\n",
    "        kf_X_test = X_train.iloc[test_index]  # éªŒè¯é›†\n",
    "        kf_y_test = y_train.iloc[test_index]\n",
    "        if clfname == 'lgbm':\n",
    "            clf.fit(kf_X_train, kf_y_train,categorical_feature=[39],early_stopping_rounds=200,eval_set=[(kf_X_test, kf_y_test)])\n",
    "        elif clfname == 'rf':\n",
    "            clf.fit(kf_X_train, kf_y_train)  # å½“å‰æ¨¡å‹è¿›è¡Œè®­ç»ƒ\n",
    "        else:\n",
    "            clf.fit(kf_X_train, kf_y_train, early_stopping_rounds=200, eval_set=[(kf_X_test, kf_y_test)])\n",
    "\n",
    "        oof_train[test_index] = clf.predict_proba(kf_X_test)# å¯¹å½“å‰éªŒè¯é›†è¿›è¡Œé¢„æµ‹\n",
    "        oof_test += np.around(clf.predict_proba(X_test),6) # å¯¹æµ‹è¯•é›†é¢„æµ‹\n",
    "\n",
    "    oof_test /= 5   # å¯¹æ¯ä¸€åˆ™äº¤å‰éªŒè¯çš„ç»“æœå–å¹³å‡\n",
    "    return oof_train, oof_test  # è¿”å›å½“å‰åˆ†ç±»å™¨å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„é¢„æµ‹ç»“æœ\n",
    "\n",
    "# å°†æ•°æ®æ¢æˆä½ çš„æ•°æ®\n",
    "X_train = x_train_1\n",
    "y_train = y_train_1\n",
    "X_test = x_test_1\n",
    "\n",
    "# å°†ä½ çš„æ¯ä¸ªåˆ†ç±»å™¨éƒ½è°ƒç”¨get_oofå‡½æ•°ï¼Œå¹¶æŠŠå®ƒä»¬çš„ç»“æœåˆå¹¶ï¼Œå°±å¾—åˆ°äº†æ–°çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®new_train,new_test\n",
    "new_train, new_test = [], []\n",
    "for clfname in ['lgbm','cat','rf']:\n",
    "    oof_train, oof_test = get_oof(clfname, X_train, y_train, X_test)\n",
    "    new_train.append(oof_train)\n",
    "    new_test.append(oof_test)\n",
    "\n",
    "new_train = np.concatenate(new_train, axis=1)\n",
    "new_test = np.concatenate(new_test, axis=1)\n",
    "new_train = pd.DataFrame(new_train)\n",
    "new_test = pd.DataFrame(new_test)\n",
    "new_train.columns = ['lgbm0','lgbm1','cat0','cat1','rf0','rf1']\n",
    "new_test.columns = ['lgbm0','lgbm1','cat0','cat1','rf0','rf1']\n",
    "new_train = new_train[['lgbm1','cat1','rf1']]\n",
    "new_test = new_test[['lgbm1','cat1','rf1']]\n",
    "\n",
    "# ç”¨æ–°çš„è®­ç»ƒæ•°æ®new_trainä½œä¸ºæ–°çš„æ¨¡å‹çš„è¾“å…¥ï¼Œstackingç¬¬äºŒå±‚\n",
    "x_train4, y_train4 = new_train, y_train\n",
    "\n",
    "def objective(trial):\n",
    "    param_grid = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 5),\n",
    "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"]),\n",
    "        \"class_weight\": \"balanced\",\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=72)\n",
    "    scores = []\n",
    "    # åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯åŠè¿›è¡Œå‚æ•°ä¼˜åŒ–\n",
    "    for train_index, test_index in skf.split(x_train4, y_train4):\n",
    "        x_train5, x_test5 = x_train4.iloc[train_index], x_train4.iloc[test_index]\n",
    "        y_train5, y_test5 = y_train4.iloc[train_index], y_train4.iloc[test_index]\n",
    "\n",
    "        model = RandomForestClassifier(**param_grid)\n",
    "        model.fit(x_train5, y_train5)\n",
    "        preds = model.predict_proba(x_test5)[:, 1]\n",
    "        score = balanced_logarithmic_loss_new(preds, y_test5)\n",
    "        print(score)\n",
    "        scores.append(score)\n",
    "\n",
    "    func_out = np.mean(scores)\n",
    "\n",
    "    return func_out\n",
    "study = optuna.create_study(study_name=f'stacking_study_rf',direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=200)\n",
    "best_params = study.best_params\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in best_params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "# è®°å½•å¼€å§‹æ—¶é—´\n",
    "start_time = time.time()\n",
    "\n",
    "stacking_model = RandomForestClassifier(**best_params)\n",
    "stacking_model.fit(new_train, y_train)\n",
    "# è®¡ç®—è®­ç»ƒæ—¶é—´\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"training time is {training_time}\")\n",
    "\n",
    "prediction = stacking_model.predict_proba(new_test)[:, 1]\n",
    "score_test = balanced_logarithmic_loss_new(prediction, y_test_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101a8b43",
   "metadata": {},
   "source": [
    "### 3.2.4 åŸºå­¦ä¹ å™¨ï¼šLGBM, CatBoost, XGBoost å…ƒå­¦ä¹ å™¨ï¼šRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046adaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_N_FOLDS = 5\n",
    "_N_CLASS = 2\n",
    "kf = KFold(n_splits=_N_FOLDS, shuffle=True, random_state=68)\n",
    "\n",
    "def get_oof(clfname, X_train, y_train, X_test):\n",
    "    if clfname == 'xgb':\n",
    "        bestparams_xgb = {\n",
    "            'n_estimators': 19808,\n",
    "            'learning_rate': 0.29003009973815874,\n",
    "            'max_depth': 11,\n",
    "            'gamma': 0.3584743148039589,\n",
    "            'subsample': 0.7,\n",
    "            'colsample_bytree': 0.6874234399659787,\n",
    "            'reg_alpha': 0.14303443256590267,\n",
    "            'reg_lambda': 0.30450443282388806,\n",
    "            'min_child_weight': 2.78622578591918,\n",
    "            'max_delta_step': 6,\n",
    "            'alpha': 0.43245457551855127,\n",
    "        }\n",
    "        clf = xgb.XGBClassifier(**bestparams_xgb,feval = balanced_logarithmic_loss_new)\n",
    "\n",
    "    if clfname == 'cat':\n",
    "        bestparams_cat = {\n",
    "            'bagging_temperature': 7.442784574066854,\n",
    "            'border_count': 212,\n",
    "            'depth': 3,\n",
    "            'iterations': 689,\n",
    "            'l2_leaf_reg': 0.8262450118748192,\n",
    "            'learning_rate': 0.09468235278046022,\n",
    "            'random_strength': 3.098327157242888\n",
    "        }\n",
    "        clf = CatBoostClassifier(**bestparams_cat,auto_class_weights='Balanced')\n",
    "\n",
    "    if clfname == 'lgbm':\n",
    "        bestparams_lgbm = {\n",
    "            'bagging_fraction': 0.9,\n",
    "            'bagging_freq': 1,\n",
    "            'colsample_bytree': 0.8755453466152356,\n",
    "            'feature_fraction': 0.6000000000000001,\n",
    "            'learning_rate': 0.2941129864759795,\n",
    "            'max_depth': 4,\n",
    "            'min_child_samples': 85,\n",
    "            'n_estimators': 22976,\n",
    "            'num_leaves': 2870,\n",
    "            'reg_alpha': 0.515407087957732,\n",
    "            'reg_lambda': 0.26493493680587554\n",
    "        }\n",
    "        clf = lgb.LGBMClassifier(**bestparams_lgbm, class_weight='balanced')\n",
    "    # X_train: *\n",
    "    # y_train: 1 *\n",
    "    # X_test :  *\n",
    "    oof_train = np.zeros((X_train.shape[0], _N_CLASS))  # Stackingåè®­ç»ƒæ•°æ®çš„è¾“å‡º\n",
    "    oof_test = np.zeros((X_test.shape[0], _N_CLASS))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X_train)): # äº¤å‰éªŒè¯åˆ’åˆ†æ­¤æ—¶çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "        kf_X_train = X_train.iloc[train_index]  # è®­ç»ƒé›†\n",
    "        kf_y_train = y_train.iloc[train_index]  # è®­ç»ƒé›†å¯¹åº”çš„è¾“å‡º\n",
    "        kf_X_test = X_train.iloc[test_index]  # éªŒè¯é›†\n",
    "        kf_y_test = y_train.iloc[test_index]\n",
    "        if clfname == 'lgbm':\n",
    "            clf.fit(kf_X_train, kf_y_train,categorical_feature=[39],early_stopping_rounds=200,eval_set=[(kf_X_test, kf_y_test)])\n",
    "        else:\n",
    "            clf.fit(kf_X_train, kf_y_train,early_stopping_rounds=200,eval_set=[(kf_X_test, kf_y_test)])  # å½“å‰æ¨¡å‹è¿›è¡Œè®­ç»ƒ\n",
    "\n",
    "        oof_train[test_index] = clf.predict_proba(kf_X_test)# å¯¹å½“å‰éªŒè¯é›†è¿›è¡Œé¢„æµ‹\n",
    "        oof_test += np.around(clf.predict_proba(X_test),6) # å¯¹æµ‹è¯•é›†é¢„æµ‹\n",
    "\n",
    "    oof_test /= 5   # å¯¹æ¯ä¸€åˆ™äº¤å‰éªŒè¯çš„ç»“æœå–å¹³å‡\n",
    "    return oof_train, oof_test  # è¿”å›å½“å‰åˆ†ç±»å™¨å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„é¢„æµ‹ç»“æœ\n",
    "\n",
    "# å°†æ•°æ®æ¢æˆä½ çš„æ•°æ®\n",
    "X_train = x_train_1\n",
    "y_train = y_train_1\n",
    "X_test = x_test_1\n",
    "\n",
    "# å°†ä½ çš„æ¯ä¸ªåˆ†ç±»å™¨éƒ½è°ƒç”¨get_oofå‡½æ•°ï¼Œå¹¶æŠŠå®ƒä»¬çš„ç»“æœåˆå¹¶ï¼Œå°±å¾—åˆ°äº†æ–°çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®new_train,new_test\n",
    "new_train, new_test = [], []\n",
    "for clfname in ['lgbm','cat','xgb']:\n",
    "    oof_train, oof_test = get_oof(clfname, X_train, y_train, X_test)\n",
    "    new_train.append(oof_train)\n",
    "    new_test.append(oof_test)\n",
    "\n",
    "new_train = np.concatenate(new_train, axis=1)\n",
    "new_test = np.concatenate(new_test, axis=1)\n",
    "new_train = pd.DataFrame(new_train)\n",
    "new_test = pd.DataFrame(new_test)\n",
    "new_train.columns = ['lgbm0','lgbm1','cat0','cat1','xgb0','xgb1']\n",
    "new_test.columns = ['lgbm0','lgbm1','cat0','cat1','xgb0','xgb1']\n",
    "new_train = new_train[['lgbm1','cat1','xgb1']]\n",
    "new_test = new_test[['lgbm1','cat1','xgb1']]\n",
    "new_train.columns = ['LGBM','CatBoost','XGB']\n",
    "\n",
    "\n",
    "# ç”¨æ–°çš„è®­ç»ƒæ•°æ®new_trainä½œä¸ºæ–°çš„æ¨¡å‹çš„è¾“å…¥ï¼Œstackingç¬¬äºŒå±‚\n",
    "\n",
    "x_train4, y_train4 = new_train, y_train\n",
    "def objective(trial):\n",
    "    param_grid = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 5),\n",
    "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"]),\n",
    "        \"class_weight\": \"balanced\",\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=72)\n",
    "    scores = []\n",
    "    # åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯åŠè¿›è¡Œå‚æ•°ä¼˜åŒ–\n",
    "    for train_index, test_index in skf.split(x_train4, y_train4):\n",
    "        x_train5, x_test5 = x_train4.iloc[train_index], x_train4.iloc[test_index]\n",
    "        y_train5, y_test5 = y_train4.iloc[train_index], y_train4.iloc[test_index]\n",
    "\n",
    "        model = RandomForestClassifier(**param_grid)\n",
    "        model.fit(x_train5, y_train5)\n",
    "        preds = model.predict_proba(x_test5)[:, 1]\n",
    "        score = balanced_logarithmic_loss_new(preds, y_test5)\n",
    "        print(score)\n",
    "        scores.append(score)\n",
    "\n",
    "    func_out = np.mean(scores)\n",
    "\n",
    "    return func_out\n",
    "study = optuna.create_study(study_name=f'stacking_study_rf',direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=200)\n",
    "best_params = study.best_params\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in best_params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "# è®°å½•å¼€å§‹æ—¶é—´\n",
    "start_time = time.time()\n",
    "\n",
    "stacking_model = RandomForestClassifier(**best_params)\n",
    "stacking_model.fit(new_train, y_train)\n",
    "\n",
    "# è®¡ç®—è®­ç»ƒæ—¶é—´\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"training time is {training_time}\")\n",
    "prediction = stacking_model.predict_proba(new_test)[:, 1]\n",
    "score_test = balanced_logarithmic_loss_new(prediction, y_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a88fa",
   "metadata": {},
   "source": [
    "## 3.3 (LGBM, CatBoost, XGBoost + RF)Stackingæ¨¡å‹æ•ˆæœåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa1eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = shap.TreeExplainer(stacking_model)\n",
    "explainer.expected_value\n",
    "np.mean(stacking_model.predict_proba(new_train)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25a14ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# å˜é‡é‡è¦æ€§ shap value\n",
    "shap_values = explainer.shap_values(new_train)\n",
    "shap.summary_plot(shap_values[1], new_train, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21971835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å˜é‡é‡è¦æ€§ PDP\n",
    "from sklearn.inspection import permutation_importance\n",
    "perimp = permutation_importance(stacking_model, new_train, y_train, n_repeats=10, random_state=1)\n",
    "perm_importance = pd.Series(perimp.importances_mean, index = new_train.columns, name = 'Var')\n",
    "perm_importance.sort_values().plot(kind='barh')\n",
    "plt.show(block = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸¤ä¸ªæœ€é‡è¦çš„å˜é‡çš„PDP\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "disp1 = PartialDependenceDisplay.from_estimator(stacking_model, new_train, features=[\"cat1\",\"lgbm1\"],method=\"brute\")\n",
    "plt.show(block = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c76b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸€ä¸ªæ ·æœ¬çš„force_plot\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][0,:], feature_names=new_train.columns,matplotlib=True)\n",
    "plt.show(block = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰€æœ‰å˜é‡å¯¹é¢„æµ‹çš„å½±å“\n",
    "shap.summary_plot(shap_values[1], new_train ,matplotlib=True)\n",
    "plt.show(block = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e3bd82",
   "metadata": {},
   "source": [
    "# 4.åŸºäºVotingçš„æ¨¡å‹èåˆæ„å»º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f16640",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_paras = {'bagging_temperature': 7.442784574066854,\n",
    "            'border_count': 212,\n",
    "            'depth': 3,\n",
    "            'iterations': 689,\n",
    "            'l2_leaf_reg': 0.8262450118748192,\n",
    "            'learning_rate': 0.09468235278046022,\n",
    "            'random_strength': 3.098327157242888\n",
    "                 }\n",
    "\n",
    "\n",
    "xgb_paras = {'n_estimators': 19808,\n",
    "            'learning_rate': 0.29003009973815874,\n",
    "            'max_depth': 11,\n",
    "            'gamma': 0.3584743148039589,\n",
    "            'subsample': 0.7,\n",
    "            'colsample_bytree': 0.6874234399659787,\n",
    "            'reg_alpha': 0.14303443256590267,\n",
    "            'reg_lambda': 0.30450443282388806,\n",
    "            'min_child_weight': 2.78622578591918,\n",
    "            'max_delta_step': 6,\n",
    "            'alpha': 0.43245457551855127\n",
    "                 }\n",
    "\n",
    "lgb_paras = {\n",
    "            'bagging_fraction': 0.9,\n",
    "            'bagging_freq': 1,\n",
    "            'colsample_bytree': 0.8755453466152356,\n",
    "            'feature_fraction': 0.6000000000000001,\n",
    "            'learning_rate': 0.2941129864759795,\n",
    "            'max_depth': 4,\n",
    "            'min_child_samples': 85,\n",
    "            'n_estimators': 22976,\n",
    "            'num_leaves': 2870,\n",
    "            'reg_alpha': 0.515407087957732,\n",
    "            'reg_lambda': 0.26493493680587554\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4a2da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('catboost', CatBoostClassifier(**catboost_paras,auto_class_weights='Balanced')),\n",
    "    ('xgb', xgb.XGBClassifier(**xgb_paras,feval = balanced_logarithmic_loss_new)),\n",
    "    ('lgb',lgb.LGBMClassifier(**lgb_paras, class_weight='balanced'))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fca54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "voting_model = VotingClassifier(models, voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed0e737",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_model.fit(x_train_1, y_train_1)\n",
    "val_preds = voting_model.predict_proba(x_test_1)\n",
    "val_score = balanced_logarithmic_loss_new(val_preds[:, 1],y_test_1)\n",
    "val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8333fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = voting_model.predict_proba(x_test_1)\n",
    "val_score = balanced_logarithmic_loss_new(val_preds[:, 1],y_test_1)\n",
    "val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab7d255",
   "metadata": {},
   "source": [
    "# 5.è¾“å‡ºæ¯”èµ›ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c83e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = stacking_model.predict_proba(new_test)\n",
    "submission_stacking = pd.DataFrame(columns = submission_df.columns)\n",
    "submission_stacking['Id'] = submission_df['Id']\n",
    "submission_stacking[['class_0','class_1']] = pred_vote\n",
    "submission_stacking.to_csv(\"submission_stacking.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
